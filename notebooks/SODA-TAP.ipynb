{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c846d65d-cfb6-4068-bd1a-426d1565eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()\n",
    "from typing import Iterator\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from string import digits \n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "import re\n",
    "import emojis\n",
    "import emoji\n",
    "import emot\n",
    "import json\n",
    "from redditscore.tokenizer import CrazyTokenizer\n",
    "import preprocessor as tp\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import TweetTokenizer, WordPunctTokenizer, MWETokenizer, word_tokenize\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, strip_short, split_alphanum\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row, Window\n",
    "from pyspark.sql.functions import udf, col, array, when, size, spark_partition_id, pandas_udf, PandasUDFType\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType, DoubleType, LongType, StructType, StructField, IntegerType, TimestampType, BooleanType, DecimalType\n",
    "from urllib3.exceptions import ProtocolError\n",
    "import sqlalchemy\n",
    "from sqlalchemy.dialects import postgresql \n",
    "import psutil\n",
    "from flashtext import KeywordProcessor\n",
    "from itertools import chain\n",
    "from textblob import TextBlob\n",
    "from decimal import Decimal\n",
    "import functools\n",
    "import operator\n",
    "import string \n",
    "import time\n",
    "import urlexpander\n",
    "from bs4 import BeautifulSoup\n",
    "import base64\n",
    "import requests \n",
    "import demoji\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "emot_obj = emot.core.emot() \n",
    "\n",
    "tp.set_options(tp.OPT.URL, tp.OPT.EMOJI, tp.OPT.RESERVED, tp.OPT.MENTION)\n",
    "cleanTweetFilter = [lambda x: x.lower(), strip_tags, strip_multiple_whitespaces, remove_stopwords, split_alphanum, strip_numeric, strip_short]\n",
    "cleanTweetAspFilter = [lambda x: x, strip_tags, strip_multiple_whitespaces, split_alphanum, remove_stopwords, strip_numeric, strip_short]\n",
    "tweetFilter = CrazyTokenizer(keepcaps=False, hashtags='HASHTAG', remove_punct=True, remove_nonunicode=True, ignore_stopwords=True, remove_breaks=True, urls='URL', twitter_handles='HANDLE', normalize=2)\n",
    "aspFilter = CrazyTokenizer(keepcaps=True, hashtags='split', remove_punct=True, remove_nonunicode=True, ignore_stopwords=True, remove_breaks=True, urls=False, normalize=2)\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "punct = str.maketrans(dict.fromkeys(string.punctuation.replace(\"'\", \"\")))\n",
    "punct2 = str.maketrans(dict.fromkeys(string.punctuation.replace('!', '').replace('?', '').replace('_', '')))\n",
    "characters = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "tweetTokenizer = TweetTokenizer()\n",
    "punctTokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72849fa6-5cd4-40f2-a414-668e73449cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionaries/aspects.json') as aspectsFile:\n",
    "    aspects = json.load(aspectsFile)\n",
    "    aspectsKeys = list(aspects.keys())\n",
    "    all_aspects = set(list(aspects) + list(chain.from_iterable(aspects.values())))\n",
    "        \n",
    "with open('dictionaries/pv.json') as pvFile:\n",
    "    pvDictionary = json.load(pvFile)\n",
    "    \n",
    "with open('dictionaries/humour.json') as humourFile:\n",
    "    humourDictionary = json.load(humourFile)\n",
    "    \n",
    "with open('dictionaries/sentiment.json') as sentimentFile:\n",
    "    sentimentDictionary = json.load(sentimentFile)\n",
    "\n",
    "aspects_processor = KeywordProcessor()\n",
    "aspects_processor.add_keywords_from_dict(aspects)\n",
    "aspects_processor.add_non_word_boundary('-')\n",
    "\n",
    "pv_processor = KeywordProcessor()\n",
    "humour_processor = KeywordProcessor()\n",
    "sentiment_processor = KeywordProcessor()\n",
    "\n",
    "pv_processor.add_keywords_from_dict(pvDictionary)\n",
    "humour_processor.add_keywords_from_dict(humourDictionary)\n",
    "sentiment_processor.add_keywords_from_dict(sentimentDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d81841-e487-42be-8a09-7c21326a201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SoDa-TAP\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.local.dir\", \"/home/jovyan/sodatap\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\") \\\n",
    "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .config('spark.streaming.kafka.consumer.cache.enabled', 'false') \\\n",
    "    .config('spark.kryoserializer.buffer.max', '2000M') \\\n",
    "    .config('spark.driver.maxResultSize', '1G') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad2d20-4075-4e21-8025-52a6dc06a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074088f-0ba1-4c35-90f5-1d72fe7c503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixText(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    text = soup.text\n",
    "    return text\n",
    "\n",
    "def tokenize_tweet(text):\n",
    "    tweetTokens = tweet_tokenizer.tokenize(text)\n",
    "    return tweetTokens\n",
    "\n",
    "def clean_tweet_text(text):\n",
    "    if text is not None:\n",
    "        text = fixText(text)\n",
    "        cleanText = ' '.join(re.split('_+', text))\n",
    "        cleanText = tweetFilter.tokenize(text)\n",
    "        cleanText = ' '.join([word for word in cleanText if word != \"HASHTAG\" and word != \"HANDLE\" and word != \"URL\"])\n",
    "        cleanText = ' '.join(preprocess_string(cleanText, cleanTweetFilter))\n",
    "        return cleanText\n",
    "    \n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "            return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "            return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "            return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "            return 'r'\n",
    "    else:\n",
    "    # As default pos in lemmatization is Noun\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatize_tweet_text(text):\n",
    "    if text is not None:\n",
    "        list_pos = 0\n",
    "        lemmatizedText = ''\n",
    "        text = text.split()\n",
    "        tagged_words = nltk.pos_tag(text)\n",
    "        for word in tagged_words:\n",
    "            lemma = lmtzr.lemmatize(word[0], get_wordnet_pos(word[1]))\n",
    "            if list_pos == 0:\n",
    "                lemmatizedText = lemma\n",
    "            else:\n",
    "                lemmatizedText = lemmatizedText + ' ' + lemma\n",
    "            list_pos += 1\n",
    "        return lemmatizedText\n",
    "\n",
    "def clean_asp_text(text):\n",
    "    if text is not None:\n",
    "        text = fixText(text)\n",
    "        cleanText = ' '.join(re.split('_+', text))\n",
    "        emoticons = emot_obj.emoticons(cleanText)\n",
    "        for i in range(0, len(emoticons['mean'])):\n",
    "            emoticons['mean'][i] = emoticons['mean'][i].replace(' ','_').replace(',', '').lower()\n",
    "        emoticonsDict = dict(zip(emoticons['value'], emoticons['mean']))\n",
    "        cleanText = cleanText.split()\n",
    "        for i in range(0, len(cleanText)):\n",
    "            if cleanText[i] in emoticonsDict:\n",
    "                cleanText[i] = emoticonsDict[cleanText[i]]\n",
    "        cleanText = ' '.join(cleanText)\n",
    "        cleanText = tp.clean(cleanText)\n",
    "        cleanText = preprocess_string(cleanText, cleanTweetAspFilter)\n",
    "        signs = {}\n",
    "        counter = 0\n",
    "        for word in cleanText:\n",
    "            if word.endswith('!') or word.endswith('?'):\n",
    "                index = cleanText.index(word)\n",
    "                signs[\"withsign\"+str(counter)] = word.lstrip(string.punctuation)\n",
    "                cleanText[index] = \"withsign\"+str(counter)\n",
    "                counter+=1\n",
    "        cleanText = ' '.join(cleanText)\n",
    "        cleanText = cleanText.translate(characters)\n",
    "        cleanText = aspFilter.tokenize(cleanText)\n",
    "        signsKeys = signs.keys()\n",
    "        for word in cleanText:\n",
    "            if word in signsKeys:\n",
    "                index = cleanText.index(word)\n",
    "                cleanText[index] = signs[word]\n",
    "        emoticonsDictKeys = emoticonsDict.keys()\n",
    "        for i in range(0, len(cleanText)):\n",
    "            if cleanText[i] in emoticonsDict.values():\n",
    "                cleanText[i] = list(emoticonsDict.keys())[list(emoticonsDict.values()).index(cleanText[i])]\n",
    "        cleanText = ' '.join(cleanText)\n",
    "        return cleanText\n",
    "\n",
    "def get_main_aspect(found_a):\n",
    "    dictionaryProcessor = aspects_processor\n",
    "    if found_a in dictionaryProcessor:\n",
    "        return dictionaryProcessor[found_a]\n",
    "    elif found_a in aspectsKeys:\n",
    "        return found_a\n",
    "    return None\n",
    "\n",
    "def get_aspects(clean_asp):\n",
    "    if clean_asp is not None:\n",
    "        aspect_sentiment = []\n",
    "        compoundWords = []\n",
    "        matches = aspects_processor.extract_keywords(clean_asp, span_info=True)\n",
    "        repeatedMatches = {}\n",
    "        for key, start, end in matches:\n",
    "            word = clean_asp[start:end].split()\n",
    "            if len(word) >= 2:\n",
    "                compoundWords.append(word)\n",
    "        compoundAsOneTokenizer = MWETokenizer(compoundWords, separator=' ')\n",
    "        doc = compoundAsOneTokenizer.tokenize(clean_asp.split())\n",
    "        #print(clean_asp)\n",
    "        #print(doc)\n",
    "        #print(matches)\n",
    "        for key, start, end in matches:\n",
    "            span = clean_asp[start:end]\n",
    "            for i in range(0, len(doc)):                \n",
    "                if doc[i].endswith('!') or doc[i].endswith('?'):\n",
    "                    doc[i] = doc[i][:-1]\n",
    "            nouns = TextBlob(span).noun_phrases\n",
    "            if span not in repeatedMatches:\n",
    "                indexes = [n for n,x in enumerate(doc) if x==span]\n",
    "                repeatedMatches[span] = indexes\n",
    "        for key, value in repeatedMatches.items():\n",
    "            span = key\n",
    "            for index in value:\n",
    "                spanIndex = index\n",
    "                # get lefts and rights for phrase if dep parse only includes aspect\n",
    "                if len(nouns) <= len(span.split()):\n",
    "                    phrase = ' '.join([t for t in doc[max(0, spanIndex-3):(spanIndex+1)+3]])\n",
    "                else:\n",
    "                    phrase = ' '.join([t for t in nouns])\n",
    "                # if we decide to use TextBlob ... polarity [-1.0, 1.0] subjectivity [0.0, 1.0]\n",
    "                # polarity, subjectivity = TextBlob(tweet).sentiment\n",
    "                # aspect_dict = {'aspect': token.text, 'phrase': phrase, 'polarity': polarity, 'subjectivity': subjectivity}\n",
    "                # if we keep using Vader ...\n",
    "                main_aspect = get_main_aspect(span.lower())\n",
    "                if main_aspect != None:\n",
    "                    aspect_dict = {'main_aspect': main_aspect, 'found_aspect': span, 'phrase': phrase}\n",
    "                    aspect_dict.update(analyzer.polarity_scores(phrase))\n",
    "                    aspect_sentiment.append(json.dumps(aspect_dict))\n",
    "        return aspect_sentiment\n",
    "\n",
    "def find_hashtags(text):\n",
    "    if text != None:\n",
    "        hashtags = re.findall(r\"#(\\w+)\", text)\n",
    "        return hashtags \n",
    "\n",
    "def find_handles(text):\n",
    "    if text is not None:\n",
    "        handles = re.findall(r\"@(\\w+)\", text)\n",
    "        return handles \n",
    "\n",
    "def find_urls_in_text(text):\n",
    "    if text is not None:\n",
    "        regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "        url = re.findall(regex, text)   \n",
    "        urls = [x[0] for x in url]\n",
    "        return urls\n",
    "    \n",
    "def find_emojis(text):\n",
    "    if text is not None:\n",
    "        emojis = demoji.findall_list(text, desc=False)\n",
    "        return emojis\n",
    "\n",
    "def get_sentiment(text):\n",
    "    if text is not None:\n",
    "        sentiment = analyzer.polarity_scores(text)['compound']\n",
    "        return sentiment\n",
    "\n",
    "def text_freq(text):\n",
    "    if text is not None:\n",
    "        textFreq = {}\n",
    "        if len(text) > 0:\n",
    "            textFreq = dict(Counter(text.split(' ')))\n",
    "            #print(textFreq)\n",
    "            return json.dumps(textFreq)\n",
    "        return json.dumps({})\n",
    "\n",
    "def keyword_search(text, dictType):\n",
    "    if text is not None and dictType is not None:\n",
    "        matcher = {}\n",
    "        if dictType == \"pv\":\n",
    "            dictionaryProcessor = pv_processor\n",
    "        elif dictType == \"humour\":\n",
    "            dictionaryProcessor = humour_processor\n",
    "        elif dictType == \"sentiment\":\n",
    "            dictionaryProcessor = sentiment_processor\n",
    "\n",
    "        words = json.loads(text_freq(text))\n",
    "        for key in words:\n",
    "            if key in dictionaryProcessor:\n",
    "                if dictionaryProcessor[key] not in matcher:\n",
    "                    matcher[dictionaryProcessor[key]] = []\n",
    "                matcher[dictionaryProcessor[key]].append([key, words[key]])\n",
    "            else:\n",
    "                continue\n",
    "        return json.dumps(matcher)\n",
    "\n",
    "def count_word_freq(wordsList):\n",
    "    if wordsList is not None:\n",
    "        counter = 0\n",
    "        for key in wordsList:\n",
    "            counter += sum(wordsList[key]['count'])\n",
    "        return counter\n",
    "\n",
    "def count_elements(elements):\n",
    "    if elements is not None:\n",
    "        return len(elements)\n",
    "    \n",
    "def calculate_engagement(retweet_count, like_count, quote_count, reply_count, user_followers_count):    \n",
    "    if retweet_count != None and like_count != None and quote_count != None and reply_count != None and user_followers_count != None:\n",
    "        engagement = 0\n",
    "        if user_followers_count > 0:\n",
    "            engagement = ((retweet_count + like_count + quote_count + reply_count) / user_followers_count) * 100\n",
    "        return engagement\n",
    "\n",
    "def calculate_extendedReach(retweet_count, user_tweet_count):\n",
    "    if retweet_count is not None and user_tweet_count is not None:\n",
    "        extendedReach = 0\n",
    "        if user_tweet_count > 0:\n",
    "            extendedReach = (retweet_count / user_tweet_count) * 100\n",
    "        return extendedReach\n",
    "\n",
    "def calculate_impressions(user_followers_count, user_tweet_count):\n",
    "    if user_followers_count is not None and user_tweet_count is not None:\n",
    "        impressions = multiply(user_followers_count, user_tweet_count)\n",
    "        return impressions\n",
    "    \n",
    "def create_media_table_query(query):\n",
    "    global dbName\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    dbUrl = 'http://{}:4200/_sql'.format(dbName)\n",
    "    requests.post(dbUrl, headers=headers, data=query)\n",
    "\n",
    "def create_photos_table():\n",
    "    createCommand = '{\"stmt\": \"create blob table photos clustered into 3 shards with (number_of_replicas=0)\"}'\n",
    "    create_media_table_query(createCommand)\n",
    "\n",
    "def save_photos_hashes(urls):\n",
    "    allPhotosHashes = []\n",
    "    if len(urls) > 0:\n",
    "        for url in urls:\n",
    "            try:\n",
    "                photo = base64.b64encode(requests.get(url).content)\n",
    "                photoToStr = photo.decode('ascii')\n",
    "                photoHash = hashlib.sha1(photoToStr.encode(\"utf-8\")).hexdigest()\n",
    "                dbUrl = 'http://{}:4200/_blobs/photos/'.format('cratedb')+photoHash\n",
    "                requests.put(dbUrl, data=photoToStr) #put blob in table\n",
    "                allPhotosHashes.append(photoHash)\n",
    "            except:\n",
    "                continue\n",
    "    return allPhotosHashes\n",
    "\n",
    "#taken from https://www.geeksforgeeks.org/sum-two-large-numbers/\n",
    "def multiply(num1, num2):\n",
    "    num1 = str(num1)\n",
    "    num2 = str(num2)\n",
    "\n",
    "    len1 = len(num1)\n",
    "    len2 = len(num2)\n",
    "    if len1 == 0 or len2 == 0:\n",
    "        return \"0\"\n",
    "\n",
    "    # will keep the result number in vector\n",
    "    # in reverse order\n",
    "    result = [0] * (len1 + len2)\n",
    "    \n",
    "    # Below two indexes are used to\n",
    "    # find positions in result.\n",
    "    i_n1 = 0\n",
    "    i_n2 = 0\n",
    "\n",
    "    # Go from right to left in num1\n",
    "    for i in range(len1 - 1, -1, -1):\n",
    "        carry = 0\n",
    "        n1 = ord(num1[i]) - 48\n",
    "\n",
    "        # To shift position to left after every\n",
    "        # multiplication of a digit in num2\n",
    "        i_n2 = 0\n",
    "\n",
    "        # Go from right to left in num2\n",
    "        for j in range(len2 - 1, -1, -1):\n",
    "            \n",
    "            # Take current digit of second number\n",
    "            n2 = ord(num2[j]) - 48\n",
    "        \n",
    "            # Multiply with current digit of first number\n",
    "            # and add result to previously stored result\n",
    "            # at current position.\n",
    "            summ = n1 * n2 + result[i_n1 + i_n2] + carry\n",
    "\n",
    "            # Carry for next iteration\n",
    "            carry = summ // 10\n",
    "\n",
    "            # Store result\n",
    "            result[i_n1 + i_n2] = summ % 10\n",
    "\n",
    "            i_n2 += 1\n",
    "\n",
    "            # store carry in next cell\n",
    "        if (carry > 0):\n",
    "            result[i_n1 + i_n2] += carry\n",
    "\n",
    "            # To shift position to left after every\n",
    "            # multiplication of a digit in num1.\n",
    "        i_n1 += 1\n",
    "        \n",
    "        # print(result)\n",
    "\n",
    "    # ignore '0's from the right\n",
    "    i = len(result) - 1\n",
    "    while (i >= 0 and result[i] == 0):\n",
    "        i -= 1\n",
    "\n",
    "    # If all were '0's - means either both or\n",
    "    # one of num1 or num2 were '0'\n",
    "    if (i == -1):\n",
    "        return \"0\"\n",
    "\n",
    "    # generate the result string\n",
    "    s = \"\"\n",
    "    while (i >= 0):\n",
    "        s += chr(result[i] + 48)\n",
    "        i -= 1\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e9fe3-75e7-476d-8429-dbb18a8c7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(StringType())\n",
    "def apply_clean_tweet_text(vector: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': vector})\n",
    "    text_df['clean_text'] = text_df['text'].apply(clean_tweet_text)\n",
    "    return text_df['clean_text']\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def apply_lemmatize_tweet_text(vector: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': vector})\n",
    "    text_df['lemmatized_text'] = text_df['text'].apply(lemmatize_tweet_text)\n",
    "    return text_df['lemmatized_text']\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def apply_clean_asp_text(vector: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': vector})\n",
    "    text_df['clean_asp_text'] = text_df['text'].apply(clean_asp_text)\n",
    "    return text_df['clean_asp_text']\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def apply_get_aspects(clean_asp: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'clean_asp': clean_asp})\n",
    "    text_df['aspect_sentiment'] = text_df['clean_asp'].apply(get_aspects)\n",
    "    return text_df['aspect_sentiment']\n",
    "\n",
    "@pandas_udf(FloatType())\n",
    "def apply_get_sentiment(vector: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': vector})\n",
    "    text_df['text_sentiment'] = text_df['text'].apply(get_sentiment)\n",
    "    text_df['text_sentiment'] = text_df['text_sentiment'].round(3)\n",
    "    return text_df['text_sentiment']\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def apply_text_freq(vector: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': vector})\n",
    "    text_df['text_freq'] = text_df['text'].apply(text_freq)\n",
    "    return text_df['text_freq']\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def apply_dictionary_search(text: pd.Series, dictType: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': text, 'dictType': dictType})\n",
    "    text_df['dict_search'] = text_df.apply(lambda x: keyword_search(x.text, x.dictType), axis=1)\n",
    "    return text_df['dict_search']\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def apply_find_hashtags(vector: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': vector})\n",
    "    text_df['hashtags'] = text_df['text'].apply(find_hashtags)\n",
    "    return text_df['hashtags']\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def apply_find_handles(vector: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': vector})\n",
    "    text_df['handles'] = text_df['text'].apply(find_handles)\n",
    "    return text_df['handles']\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def apply_find_emojis(vector: pd.Series) -> pd.Series:\n",
    "    emojis = vector.apply(find_emojis)\n",
    "    return emojis\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def apply_find_urls(vector: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': vector})\n",
    "    text_df['urls'] = text_df['text'].apply(find_urls)\n",
    "    return text_df['urls']\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def apply_expand_urls(vector: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': vector})\n",
    "    text_df['urls'] = text_df['text'].apply(expand_urls)\n",
    "    return text_df['urls']\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def apply_unshorten_urls(vector: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': vector})\n",
    "    text_df['unshorten_urls'] = text_df['text'].apply(unshorten_urls)\n",
    "    return text_df['unshorten_urls']\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def apply_unshorten_urls(vector: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': vector})\n",
    "    text_df['unshorten_urls'] = text_df['text'].apply(unshorten_urls)\n",
    "    return text_df['unshorten_urls']\n",
    "\n",
    "@pandas_udf(DoubleType())\n",
    "def apply_calculate_engagement(retweet_count: pd.Series, like_count: pd.Series, quote_count: pd.Series, reply_count: pd.Series, user_followers_count: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'retweet_count': retweet_count, 'like_count': like_count, 'quote_count': quote_count, 'reply_count': reply_count, 'user_followers_count': user_followers_count})\n",
    "    text_df['engagement'] = text_df.apply(lambda x: calculate_engagement(x.retweet_count, x.like_count, x.quote_count, x.reply_count, x.user_followers_count), axis=1)\n",
    "    text_df['engagement'] = text_df['engagement'].round(3)\n",
    "    return text_df['engagement']\n",
    "    \n",
    "@pandas_udf(DoubleType())\n",
    "def apply_calculate_extendedReach(retweet_count: pd.Series, user_tweet_count: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'retweet_count': retweet_count, 'user_tweet_count': user_tweet_count})\n",
    "    text_df['extendedReach'] = text_df.apply(lambda x: calculate_extendedReach(x.retweet_count, x.user_tweet_count), axis=1)\n",
    "    text_df['extendedReach'] = text_df['extendedReach'].round(3)\n",
    "    return text_df['extendedReach']\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def apply_calculate_impressions(user_followers_count: pd.Series, user_tweet_count: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'user_followers_count': user_followers_count, 'user_tweet_count': user_tweet_count})\n",
    "    text_df['impressions'] = text_df.apply(lambda x: calculate_impressions(x.user_followers_count, x.user_tweet_count), axis=1)\n",
    "    return text_df['impressions']\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def apply_save_photos_hashes(vector: pd.Series) -> pd.Series:\n",
    "    photos_df = pd.DataFrame({'text': vector})\n",
    "    photos_df['photos_hashes'] = photos_df['text'].apply(save_photos_hashes)\n",
    "    return photos_df['photos_hashes']\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def apply_save_data(vector: pd.Series) -> pd.Series:\n",
    "    text_df = pd.DataFrame({'text': vector})\n",
    "    text_df['sent'] = text_df['text'].apply(save_data)\n",
    "    return text_df['sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd0417-3adb-42dc-af41-aeff0fa674f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sink(df, epoch_id):\n",
    "    socialData_df = df.toPandas()\n",
    "    socialData_df.to_sql(topic+\"_processed\", 'crate://cratedb:4200', if_exists='append', index=False, chunksize=10000, dtype={'text_hashtags':postgresql.ARRAY(sqlalchemy.types.String), 'text_handles':postgresql.ARRAY(sqlalchemy.types.String), 'text_emojis':postgresql.ARRAY(sqlalchemy.types.String), 'urls':postgresql.ARRAY(sqlalchemy.types.String), 'images':postgresql.ARRAY(sqlalchemy.types.String), 'aspect_sentiment':postgresql.ARRAY(sqlalchemy.types.String)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee16b5-3eb5-4e07-b4fe-1d0f8567dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_topic_schema_json(topic):\n",
    "    df_json = (spark.read.format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "                .option(\"subscribe\", topic) \\\n",
    "                .option(\"startingOffsets\", \"earliest\") \\\n",
    "                .option(\"maxOffsetsPerTrigger\", \"1\") \\\n",
    "                .option(\"failOnDataLoss\", \"false\") \\\n",
    "                .load() \\\n",
    "                .withColumn(\"value\", F.expr(\"string(value)\")) \\\n",
    "                .select(\"value\"))\n",
    "    \n",
    "    df_read = spark.read.json(df_json.rdd.map(lambda x: x.value), multiLine=True)\n",
    "    return df_read.schema.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a60057-a92c-4815-8e15-1391cd491ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_schema = False\n",
    "schema_location = \"schemas/tweets.json\"\n",
    "\n",
    "if not infer_schema: \n",
    "    try:\n",
    "        with open(schema_location, 'r') as f:\n",
    "            topic_schema_txt = json.load(f)\n",
    "    except:\n",
    "        infer_schema = True\n",
    "        pass\n",
    "\n",
    "if infer_schema:\n",
    "    topic_schema_txt = infer_topic_schema_json(topic)\n",
    "    with open(schema_location, 'w') as f:\n",
    "        json.dump(topic_schema_txt, f)\n",
    "        \n",
    "topic_schema = StructType.fromJson(json.loads(topic_schema_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97cb4f-bb5a-4018-9374-8bc566d95c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_tweets = spark.readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "            .option(\"subscribe\", topic) \\\n",
    "            .option(\"maxOffsetsPerTrigger\", \"100000\") \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .option(\"failOnDataLoss\", \"false\") \\\n",
    "            .load() \\\n",
    "            .withColumn(\"value\", F.expr(\"string(value)\")) \\\n",
    "            .select(\"value\") \\\n",
    "            .withColumn('value', F.from_json(col(\"value\"), topic_schema)) \\\n",
    "            .select(\"value.payload.*\")\n",
    "\n",
    "\n",
    "csv_tweets = csv_tweets.repartition(64)\n",
    "\n",
    "#CSV header\n",
    "'''\n",
    "id,\n",
    "tweet,\n",
    "created_at,\n",
    "lang,\n",
    "like_count,\n",
    "quote_count,\n",
    "quoted,\n",
    "replied,\n",
    "threaded,\n",
    "reply_count,\n",
    "retweet_count,\n",
    "source,\n",
    "tweet_type,\n",
    "author_id,\n",
    "author_name,\n",
    "author_created_at,\n",
    "author_bio,\n",
    "author_followers_count,\n",
    "author_following_count,\n",
    "author_listed_count,\n",
    "author_profile_image,\n",
    "author_tweet_count,\n",
    "author_username,\n",
    "author_verified\n",
    "'''\n",
    "\n",
    "csv_tweets = csv_tweets.withColumn('text_clean', apply_clean_tweet_text('tweet'))\\\n",
    "    .withColumn('text_preprocessed', apply_lemmatize_tweet_text('text_clean'))\\\n",
    "    .withColumn('retweet_count', col('retweet_count').cast(IntegerType()))\\\n",
    "    .withColumn('reply_count', col('reply_count').cast(IntegerType()))\\\n",
    "    .withColumn('like_count', col('like_count').cast(IntegerType()))\\\n",
    "    .withColumn('quote_count', col('quote_count').cast(IntegerType()))\\\n",
    "    .withColumn('created_at', col('created_at').cast(TimestampType()))\\\n",
    "    .withColumn('author_created_at', col('author_created_at').cast(TimestampType()))\\\n",
    "    .withColumn('author_followers_count', col('author_followers_count').cast(IntegerType()))\\\n",
    "    .withColumn('author_following_count', col('author_following_count').cast(IntegerType()))\\\n",
    "    .withColumn('author_tweet_count', col('author_tweet_count').cast(IntegerType()))\\\n",
    "    .withColumn('author_listed_count', col('author_listed_count').cast(IntegerType()))\\\n",
    "    .withColumn('author_verified', col('author_verified').cast(BooleanType()))\\\n",
    "    .withColumn('text_hashtags', apply_find_hashtags('tweet'))\\\n",
    "    .withColumn('hashtags_count', size('text_hashtags'))\\\n",
    "    .withColumn('text_handles', apply_find_handles('tweet'))\\\n",
    "    .withColumn('handles_count', size('text_handles'))\\\n",
    "    .withColumn('text_emojis', apply_find_emojis('tweet'))\\\n",
    "    .withColumn('emojis_count', size('text_emojis'))\\\n",
    "    .withColumn('text_asp_preprocessed', apply_clean_asp_text('tweet'))\\\n",
    "    .withColumn('aspect_sentiment', apply_get_aspects('text_asp_preprocessed'))\\\n",
    "    .withColumn('text_sentiment', apply_get_sentiment('text_asp_preprocessed'))\\\n",
    "    .withColumn('text_preprocessed_freq', apply_text_freq('text_preprocessed'))\\\n",
    "    .withColumn('text_pv_freq', apply_dictionary_search('text_preprocessed', func.lit('pv')))\\\n",
    "    .withColumn('text_humour_freq', apply_dictionary_search(col('text_preprocessed'), func.lit('humour')))\\\n",
    "    .withColumn('text_sentiment_freq', apply_dictionary_search(col('text_preprocessed'), func.lit('sentiment')))\\\n",
    "     .withColumn('engagement_rate', apply_calculate_engagement('retweet_count', 'like_count', 'quote_count', 'reply_count', 'author_followers_count'))\\\n",
    "    .withColumn('extended_reach', apply_calculate_extendedReach('retweet_count', 'author_tweet_count'))\\\n",
    "    .withColumn('possible_impressions', apply_calculate_impressions('author_followers_count', 'author_tweet_count'))\n",
    "\n",
    "#csv_tweets = csv_tweets.na.fill(\"\")\n",
    "#.drop(col(\"text_clean\"))\n",
    "\n",
    "#if urlAnalysis:\n",
    "    \n",
    "\n",
    "#debug_sink = csv_tweets.writeStream \\\n",
    "#    .outputMode(\"update\") \\\n",
    "#    .trigger(processingTime='1 seconds') \\\n",
    "#    .option(\"truncate\", \"false\")\\\n",
    "#    .format(\"console\") \\\n",
    "#    .start()\n",
    "\n",
    "debug_sink = csv_tweets.writeStream \\\n",
    "    .foreachBatch(write_sink) \\\n",
    "    .start()\n",
    "\n",
    "debug_sink.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}